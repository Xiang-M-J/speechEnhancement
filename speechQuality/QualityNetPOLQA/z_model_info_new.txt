hasa20240522_173240：局部mask+归一化输出      	mse: 0.0073,  lcc: 0.9636, srcc: 0.9361
hasa20240522_223914：局部mask+不归一化输出    	mse: 0.1209,  lcc: 0.9626, srcc: 0.9359
hasa20240523_102833：无局部mask+不归一化输出  	mse: 0.1189,  lcc: 0.9636, srcc: 0.9365
hasa20240523_204914：无局部mask+归一化输出    	mse: 0.0071,  lcc: 0.9645, srcc: 0.9365
hasa_cp20240527_001840：压缩输入幅度		mse: 0.0070,  lcc: 0.9653, srcc: 0.9366

can2dClass20240523_162129：avg norm mse损失，cls 交叉熵损失 预测为avg+cls	mse: 0.1385,  lcc: 0.9610, srcc: 0.9353
can2dClass20240523_162129：avg norm mse损失，cls 交叉熵损失 预测为 cls	mse: 0.1462,  lcc: 0.9588, srcc: 0.9377

注意 can2dClass20240524_084734 使用的优化器为Rmprop
can2dClass20240524_084734：avg norm mse损失，cls EDM 损失   预测为 avg+cls  	mse: 0.1724,  lcc: 0.9548, srcc: 0.9295
can2dClass20240524_084734：avg norm mse损失，cls EDM 损失   预测为 cls          	mse: 0.1809,  lcc: 0.9542, srcc: 0.9328

can2dClass20240524_203611：avg norm mse 损失（x10）cls 交叉熵损失，预测为 cls           mse: 0.2075,  lcc: 0.9469, srcc: 0.9341 
can2dClass20240524_203611：avg norm mse 损失（x10）cls 交叉熵损失，预测为 avg+cls       mse: 0.1583,  lcc: 0.9549, srcc: 0.9324


lstmcanClass20240524_093002：avg norm mse 损失，cls EDM损失 预测为 avg+cls	            mse: 0.1496,  lcc: 0.9529, srcc: 0.9175
lstmcanClass20240524_185704：avg norm mse 损失（x10），cls 交叉熵损失，预测为 cls       mse: 0.1581,  lcc: 0.9501, srcc: 0.9187
lstmcanClass20240524_185704：avg norm mse 损失（x10），cls 交叉熵损失，预测为 avg+cls   mse: 0.1514,  lcc: 0.9526, srcc: 0.9257


lstm_se20240521_173158  模型最后的激活函数为 softplus 函数                                MOS_SIG:2.7743  MOS_OVRL:2.3997	
lstm_se20240521_235804  模型最后的激活函数为 sigmoid 函数（应该是尝试训练mask时留下来的）   MOS_SIG:2.8192	MOS_OVRL:2.4243	
lstm_se_IAM20240522_174211  正式训练mask（IAM）                                         MOS_SIG:2.8487	MOS_OVRL:2.4443	

注意之前在使用语音质量模型训练语音增强模型时，在采用LSTM作为语音增强模型时，语音质量模型的输入有误，缺少



下面这些实验基于的数据集 wav_train_qn_rs2.list 是经过选择的数据集，保证每个区间的分数分布大致均衡, qn_input_type=1

cnnA_cp_qn20240529_173910 对应的模型为 CnnMAttn 对应的数据集为 wav_train_qn_rs2.list qn_input_type=1 归一化输出  mse: 0.0109,  lcc: 0.9383, srcc: 0.9314
hasa_cp_qn20240529_173644 对应的模型为 HASANet 不过在一开始加了一层conv1d 对应的数据集为 wav_train_qn_rs2.list qn_input_type=1 归一化输出  mse: 0.0116,  lcc: 0.9341, srcc: 0.9269
hasa_cp_qn20240529_204006 对应的模型为 HASANet 删除了 conv1d 对应的数据集为 wav_train_qn_rs2.list qn_input_type=1 归一化输出  mse: 0.0113,  lcc: 0.9354, srcc: 0.9254
hasa_cp_qn20240529_214354 对应的模型为 HASANet 用 rmsprop训练 对应的数据集为 wav_train_qn_rs2.list qn_input_type=1 归一化输出  mse: 0.0113,  lcc: 0.9359, srcc: 0.9260
hasa_cp_qn20240529_231851 将帧的损失从 FrameMse2 换成 FrameMseNorm  mse: 0.0113,  lcc: 0.9359, srcc: 0.9257
cnn_cp_qn20240529_210701  对应的模型为 Cnn 对应的数据集为 wav_train_qn_rs2.list qn_input_type=1 归一化输出  mse: 0.0116,  lcc: 0.9339, srcc: 0.9252
cnnA_cp_qn20240529_224224 将卷积块的卷积和大小改为2    mse: 0.0104,  lcc: 0.9410, srcc: 0.9326
cnnA_cp_qn20240530_205155 在 prepare 中加上了 BatchNorm1d  mse: 0.0120,  lcc: 0.9370, srcc: 0.9318 （效果并不好）
cnnA_cp_qn20240530_233249 与 cnnA_cp_qn20240529_224224 的配置基本相同 mse: 0.0104,  lcc: 0.9413, srcc: 0.9337
cnnA_cp_qn20240531_084721 将原本的3个conv层换成2个conv层，  mse: 0.0110,  lcc: 0.9376, srcc: 0.9305
cnnA_cp_qn20240531_114024 将 prepare 中kernel_size=5改为kernel_size=4       mse: 0.0105,  lcc: 0.9404, srcc: 0.9327        
cnnA_cp_qn20240531_114054 将 prepare 中kernel_size=5 改为kernel_size=4 并且加上了一层 conv  mse: 0.0102,  lcc: 0.9421, srcc: 0.9347
cnnA_cp_qn20240531_142327 将 prepare 中kernel_size=4 改为 kernel_size=7，一共 4 层 conv  mse: 0.0111,  lcc: 0.9375, srcc: 0.9302
cnnA_cp_qn20240531_142327 将 prepare 中kernel_size=4 改为 kernel_size=3，一共 4 层 conv  mse: 0.0109,  lcc: 0.9378, srcc: 0.9312
cnnA_cp_qn20240531_220403 在prepare中的conv1d中加上了dilation=2，效果不好  mse: 0.0111,  lcc: 0.9372, srcc: 0.9295
cnnA_cp_qn20240601_085425 设置与 cnnA_cp_qn20240531_114054 基本相同        mse: 0.0102,  lcc: 0.9421, srcc: 0.9347
cnnA_cp_qn20240601_110231 将计算注意力时的dropout设置为0.2                 mse: 0.0101,  lcc: 0.9431, srcc: 0.9352
cnnA_cp_qn20240601_135012 将计算注意力时的dropout设置为0.3                 mse: 0.0104,  lcc: 0.9412, srcc: 0.9336

hasa_cp_qn20240601_085331 在linear1后加了一个avgpool2d 用来减小尺度，但是效果不好 mse: 0.0128,  lcc: 0.9273, srcc: 0.9174
hasa_cp_qn20240601_140753 不使用帧的分数计算损失  mse: 0.0117,  lcc: 0.9332, srcc: 0.9252



==== loss 为 预测损失（MSE）+ 分类损失（交叉熵）
hasaClass_cp_qn20240530_001033 对应的模型为 HASAClassifier 归一化输出 输出为 cls mse: 0.1823,  lcc: 0.9380, srcc: 0.9312
hasaClass_cp_qn20240530_001033 对应的模型为 HASAClassifier 归一化输出 输出为 avg + cls mse: 0.1755,  lcc: 0.9382, srcc: 0.9307
hasaClass_cp_qn20240531_085815 将模型中的分类器和评分器中一开始的全连接层删除 输出为 cls mse: 0.1744,  lcc: 0.9408, srcc: 0.9318
hasaClass_cp_qn20240531_191859 删除模型的layer norm 输出为cls                  mse: 0.1860,  lcc: 0.9362, srcc: 0.9258
can2dClass_cp_qn20240530_001622  对应的模型为 CAN2dClass   归一化输出  mse: 0.2046,  lcc: 0.9280, srcc: 0.9188
====

hasaClass_cp_qn20240601_124549 只使用交叉熵损失训练  mse: 0.1921,  lcc: 0.9346, srcc: 0.9269


==== 将分类器中 第一个全连接层的输出改为 2 倍 linear_output
hasaClass_cp_qn20240530_122915  对应的模型为 HASAClassifier 损失只使用EDMLoss  mse: 0.1890,  lcc: 0.9363, srcc: 0.9279
hasaClass_cp_qn20240530_140944  对应的模型为 HASAClassifier 损失使用EDMLoss + norm mse  mse: 0.1816,  lcc: 0.9381, srcc: 0.9290
====

==== 将分类器中 第一个全连接层的输出改回 1 倍 linear_output
hasaClass_cp_qn20240530_153219  对应的模型为 HASAClassifier 损失使用EDMLoss + norm mse mse: 0.1767,  lcc: 0.9399, srcc: 0.9321
====
cnnClass_cp_qn20240531_142747  模型与cnnClass_cp_qn20240530_194517的不同，与cnnMAttn相似，损失为 EDMLoss + norm mse  mse: 0.1893,  lcc: 0.9378, srcc: 0.9293

注意上面的 EDMLoss 中计算缺少了根号

cnnClass_cp_qn20240530_194517 （未记录） 对应的模型为 CnnClass，损失使用 EDMLoss(加上根号) + norm mse mse: 0.2425,  lcc: 0.9294, srcc: 0.9234
hasaClass_cp_qn20240530_221024  对应的模型为 HASAClassifier 损失使用EDMLoss（加上根号） + norm mse  mse: 0.1908,  lcc: 0.9355, srcc: 0.9276
hasaClass_cp_qn20240530_234616   仅使用交叉熵损失       mse: 0.1870,  lcc: 0.9372, srcc: 0.9296

hasaClass_cp_qn20240601_110340 使用帧EDMloss 和 EDMLoss mse: 0.1897,  lcc: 0.9362, srcc: 0.9283
hasaClass_cp_qn20240601_201732 将步长改为0.5 mse: 0.2310,  lcc: 0.9355, srcc: 0.9271
hasaClass_cp_qn20240601_215235 将步长改为0.5，且不归一化输出   mse: 0.1840,  lcc: 0.9351, srcc: 0.9268

dpcrn_qsehasa20240601_151620 注意这里用的质量模型实际为 cnnA_cp_qn20240601_110231 
MOS_COL:3.0969	MOS_DISC:4.0339	MOS_LOUD:3.6941	MOS_NOISE:3.9610	MOS_REVERB:4.0386	MOS_SIG:3.3002	MOS_OVRL:2.8234


dpcrn_grlcnnA20240606_205542 质量模型使用 cnnA_cp_qn20240601_110231
MOS_COL:3.2746	MOS_DISC:4.1523	MOS_LOUD:3.5320	MOS_NOISE:3.3834	MOS_REVERB:3.8820	MOS_SIG:3.5274	MOS_OVRL:2.8992

dpcrn_grlcnnA20240607_220153 质量模型使用 cnnA_cp_qn20240607_000313 
MOS_COL:3.3651  MOS_DISC:4.0860 MOS_LOUD:3.5704 MOS_NOISE:3.2383        MOS_REVERB:3.6539       MOS_SIG:3.2686  MOS_OVRL:2.7441

dpcrn_jointcnnA20240604_082936 质量模型使用 cnnA_cp_qn20240601_110231 每次训练时都计算质量损失
MOS_COL:3.3093	MOS_DISC:4.0472	MOS_LOUD:3.5537	MOS_NOISE:3.8294	MOS_REVERB:4.0001	MOS_SIG:3.2595	MOS_OVRL:2.8109

cnnA_cp_qn20240605_165512 将归一化输出的sigmoid激活函数换成hardsigmoid函数（没什么效果）  mse: 0.0107,  lcc: 0.9395, srcc: 0.9326
cnnA_cp_qn20240607_000313 只训练了 5 个 epoch mse: 0.0127,  lcc: 0.9279, srcc: 0.9194

crn_cp_qn20240607_231722  C=256   mse: 0.0097,  lcc: 0.9451, srcc: 0.9371
crn_cp_qn20240608_001513  C=128   mse: 0.0107,  lcc: 0.9412, srcc: 0.9350
crn_cp_qn20240608_001701  C=512 LSTM 的隐层为512（原本为 256）  mse: 0.0123,  lcc: 0.9338, srcc: 0.9292
crn_cp_qn20240608_084606  C = 256 LSTM 隐层为 128，trans 的输出为 256  mse: 0.0099,  lcc: 0.9449, srcc: 0.9387
crn_cp_qn20240608_084631  C = 256 LSTM 隐层为 128，trans 的输出为 128  mse: 0.0106,  lcc: 0.9412, srcc: 0.9351
crn_cp_qn20240608_095007  C = 256 LSTM 隐层为 128，trans 的输出为 512  mse: 0.0110,  lcc: 0.9400, srcc: 0.9338
crn_cp_qn20240608_095242  C= 256 lstm 隐层为 64，trans 的输出为 128   mse: 0.0109,  lcc: 0.9415, srcc: 0.9374  
crn_cp_qn20240608_150311  C=256 lstm 隐层为128，trans 的输出为 128   mse: 0.0103,  lcc: 0.9443, srcc: 0.9387
crn_cp_qn20240608_160031  C=256 lstm 隐层为128，trans 的输出为 128   mse: 0.0103,  lcc: 0.9411, srcc: 0.9352

